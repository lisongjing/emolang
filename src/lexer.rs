use unicode_segmentation::UnicodeSegmentation;

#[derive(Debug, PartialEq)]
enum TokenType {
    Illegal,
    End,

    Assign,

    Plus,
    Minus,
    Multiply,
    Divide,

    Equal,
    GreaterThan,
    LessThan,

    And,
    Or,
    Not,

    Semicolon,
    LBrace,
    RBrace,

    Identifier,

    True,
    False,

    Number,
    String,
}

const SYMBOLS: [&str; 18] = [
    "â¬…ï¸", "â•", "â–", "âœ–ï¸", "â—", "ğŸŸ°", "â–¶ï¸", "â—€ï¸", "ğŸ”", "ğŸ”€", "â¸ï¸", "â†™ï¸", "ğŸ‘‰", "ğŸ‘ˆ", "ğŸ—¨ï¸", "ğŸ’¬",
    "âœ”ï¸", "âŒ",
];
const DIGITALS: [&str; 11] = [
    "0ï¸âƒ£", "1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£", "7ï¸âƒ£", "8ï¸âƒ£", "9ï¸âƒ£", "ğŸ”Ÿ",
];
const DOTS: [&str; 9] = ["âšª", "âš«", "ğŸŸ¤", "ğŸŸ£", "ğŸ”µ", "ğŸŸ¢", "ğŸŸ¡", "ğŸŸ ", "ğŸ”´"];
const SPACES: [&str; 4] = [" ", "\t", "\r", "\n"];

pub struct Lexer {
    input: String,
}

impl Lexer {
    pub fn new(input: String) -> Lexer {
        Lexer { input }
    }

    pub fn tokenize(&self) -> Vec<Token> {
        let chars = self.input.graphemes(true).collect::<Vec<&str>>();
        let mut pos = 0usize;
        let mut tokens = vec![];

        while pos < chars.len() {
            let char = chars[pos];
            let token = match char {
                "â¬…ï¸" => Token::from_str(TokenType::Assign, char),
                "â•" => Token::from_str(TokenType::Plus, char),
                "â–" => Token::from_str(TokenType::Minus, char),
                "âœ–ï¸" => Token::from_str(TokenType::Multiply, char),
                "â—" => Token::from_str(TokenType::Divide, char),
                "ğŸŸ°" => Token::from_str(TokenType::Equal, char),
                "â–¶ï¸" => Token::from_str(TokenType::GreaterThan, char),
                "â—€ï¸" => Token::from_str(TokenType::LessThan, char),
                "ğŸ”" => Token::from_str(TokenType::And, char),
                "ğŸ”€" => Token::from_str(TokenType::Or, char),
                "â¸ï¸" => Token::from_str(TokenType::Not, char),
                "â†™ï¸" => Token::from_str(TokenType::Semicolon, char),
                "âœ”ï¸" => Token::from_str(TokenType::True, char),
                "âŒ" => Token::from_str(TokenType::False, char),
                "ğŸ‘‰" => Token::from_str(TokenType::LBrace, char),
                "ğŸ‘ˆ" => Token::from_str(TokenType::RBrace, char),
                "ğŸ—¨ï¸" => handle_string(&chars, &mut pos),
                _ if DIGITALS.contains(&char) => handle_number(&chars, &mut pos),
                _ if SPACES.contains(&char) => {
                    pos += 1;
                    continue;
                }
                _ if is_identifier_char(char) => handle_identifier(&chars, &mut pos),
                _ => Token::from_str(TokenType::Illegal, char),
            };
            pos += 1;
            tokens.push(token);
        }
        tokens.push(Token::from(TokenType::End, String::new()));
        tokens
    }
}

fn handle_string(chars: &[&str], pos: &mut usize) -> Token {
    let mut literal = String::new();
    *pos += 1;
    while chars[*pos] != "ğŸ’¬" {
        literal.push_str(chars[*pos]);
        *pos += 1;
    }
    Token::from(TokenType::String, literal)
}

fn handle_number(chars: &[&str], pos: &mut usize) -> Token {
    let mut literal = String::from(chars[*pos]);
    while DIGITALS.contains(&chars[*pos + 1]) || DOTS.contains(&chars[*pos + 1]) {
        *pos += 1;
        literal.push_str(chars[*pos]);
    }
    Token::from(TokenType::Number, literal)
}

fn handle_identifier(chars: &[&str], pos: &mut usize) -> Token {
    let mut literal = String::from(chars[*pos]);
    while is_identifier_char(chars[*pos + 1]) {
        *pos += 1;
        literal.push_str(chars[*pos]);
    }
    Token::from(TokenType::Identifier, literal)
}

fn is_identifier_char(char: &str) -> bool {
    !SYMBOLS.contains(&char)
        && !DIGITALS.contains(&char)
        && !DOTS.contains(&char)
        && !SPACES.contains(&char)
}

#[derive(Debug, PartialEq)]
pub struct Token {
    token_type: TokenType,
    literal: String,
}

impl Token {
    fn from(token_type: TokenType, literal: String) -> Token {
        Token {
            token_type,
            literal,
        }
    }

    fn from_str(token_type: TokenType, literal: &str) -> Token {
        Self::from(token_type, String::from(literal))
    }
}

#[cfg(test)]
mod lexer_test {
    use super::*;

    #[test]
    fn test() {
        let source = String::from("ãŠ™ï¸ğŸ”¢ â¬…ï¸ 3ï¸âƒ£ âœ–ï¸ 2ï¸âƒ£ â†™ï¸ ãŠ™ï¸ğŸ”¡ â¬…ï¸ ğŸ—¨ï¸ğŸˆ¶ğŸ…°ï¸ğŸˆšğŸ…±ï¸ğŸˆ²ğŸ†ğŸ’¬ â†™ï¸");
        let target = vec![
            Token::from_str(TokenType::Identifier, "ãŠ™ï¸ğŸ”¢"),
            Token::from_str(TokenType::Assign, "â¬…ï¸"),
            Token::from_str(TokenType::Number, "3ï¸âƒ£"),
            Token::from_str(TokenType::Multiply, "âœ–ï¸"),
            Token::from_str(TokenType::Number, "2ï¸âƒ£"),
            Token::from_str(TokenType::Semicolon, "â†™ï¸"),
            Token::from_str(TokenType::Identifier, "ãŠ™ï¸ğŸ”¡"),
            Token::from_str(TokenType::Assign, "â¬…ï¸"),
            Token::from_str(TokenType::String, "ğŸˆ¶ğŸ…°ï¸ğŸˆšğŸ…±ï¸ğŸˆ²ğŸ†"),
            Token::from_str(TokenType::Semicolon, "â†™ï¸"),
            Token::from(TokenType::End, String::new()),
        ];
        let lexer = Lexer::new(source);
        assert_eq!(lexer.tokenize(), target);
    }
}
